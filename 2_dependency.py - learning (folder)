import dlt
from pyspark.sql.functions import *

# STAGING TABLE (STREAMING SOURCE)
@dlt.table(
name="staging_orders"
)
def staging_orders():
return spark.readStream.table("dltnandhini.source.orders")


# TRANSFORMED TABLE
@dlt.table(
name="transformed_orders"
)
def transformed_orders():
df = dlt.read_stream("staging_orders")
df = df.withColumn("order_status", lower(col("order_status")))
return df


# AGGREGATED TABLE
@dlt.table(
name="aggregated_orders"
)
# def aggregated_orders():
#     df = dlt.read_stream("transformed_orders")
#     df = df.groupBy("order_status").count()
#     return df
